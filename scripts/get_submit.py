import json
import cv2
import os 
import pickle
import numpy as np
import torch
from numpy import linalg as LA
import torch
import torch.nn.functional as F


def get_mean_feats1(img_feat, tacks_ids):
    mean_gallery = []
    for k in tacks_ids:
        tmp = []
        for fid in img_feat[k]:
            tmp.append(img_feat[k][fid])
        tmp = np.vstack(tmp)
        tmp = np.mean(tmp,0)
        mean_gallery.append(tmp)
    mean_gallery = np.vstack(mean_gallery)
    return mean_gallery


used_models = ["two_branch_cam_loc_dir", "motion_ibn_aug_cam", "motion_ibn_nlpaug_320_all", "motion_ibn_nlpaug", "three_branch_1024", "two_branch_cam_direction", "mot_ibn_base_v1"]
merge_weights = [4.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.]

with open("data/test_queries.json") as f:
    queries = json.load(f)
with open("data/test_tracks.json") as f:
    tracks = json.load(f)

query_ids = list(queries.keys())
tacks_ids = list(tracks.keys())
print(len(tacks_ids), len(query_ids))

img_feats = []
nlp_feats = []

for i, m in enumerate(used_models):
    img_feats.append(get_mean_feats1(pickle.load(open("output/img_feat_%s.pkl" % m,'rb')),tacks_ids))
    nlp_feats.append(pickle.load(open("output/lang_feat_%s.pkl" % m,'rb')))

# >>> Location Augmentation >>>
# `loc_dict` can be generated by running `scripts/get_location_info.py`
loc_dict = {'c005': 1, 'c014': 0, 'c029': 0, 'c003': 1, 'c017': 0, 'c035': 0, 'c013': 0, 'c027': 0, 'c038': 0, 'c016': 0, 'c002': 1, 'c021': 0, 'c019': 0, 'c030': 0, 'c020': 0, 'c026': 0, 'c010': 0, 'c004': 1, 'c033': 0, 'c012': 1, 'c001': 1, 'c034': 0, 'c022': 0, 'c036': 0, 'c040': 1, 'c025': 0, 'c032': 0, 'c037': 1}
loc_logits = dict()
for track_id, track in tracks.items():
    cam = track['frames'][0].split('/')[-3]
    if loc_dict[cam] == 1:
        loc_logits[track_id] = [0,1]
    else:
        loc_logits[track_id] = [1,0]

img_loc_logits = np.array([v for k, v in loc_logits.items()])
# <<< Location Augmentation <<<


# >>> Relationship Augmentation >>>
# The following files can be generated by running `scripts/get_relation_info.py`
with open('query_lang_embeds.pkl', 'rb') as fb:
    query_lang_embeds = pickle.load(fb)

with open('track_car_embeds.pkl', 'rb') as fb:
    track_car_embeds = pickle.load(fb)

with open('test_query_cars.json', 'r') as fb:
    test_query_cars = json.load(fb)
# <<< Relationship Augmentation <<<


def get_lang_v(texts):
    location = 0
    direction = 0

    num_left = 0
    num_right = 0
    for text in texts:
        if 'turn' in text:
            if 'left' in text:
                num_left += 1
            if 'right' in text:
                num_right += 1
        if 'intersection' in text:
            location = 1

    if num_left > num_right:
        direction = 1
    if num_left < num_right:
        direction = 2

    loc_map = [[1,0], [0,1]]
    dir_map = [[1,0,0], [0,1,0], [0,0,1]]
    return loc_map[location], dir_map[direction]


lang_loc_v = dict()
lang_dir_v = dict()
for q_id, record in queries.items():
    texts = queries[q_id]["nl"]
    loc_v, dir_v = get_lang_v(texts)
    lang_loc_v[q_id] = loc_v
    lang_dir_v[q_id] = dir_v

results = dict()

for query in query_ids:
    score = 0.
    for i in range(len(nlp_feats)):
        q = nlp_feats[i][query]
        score += merge_weights[i] * np.mean(np.matmul(q, img_feats[i].T), 0)

    loc_sim = np.matmul(np.array([lang_loc_v[query]]), img_loc_logits.T)
    loc_sim = loc_sim.T
    loc_sim = np.squeeze(loc_sim)
    # add location similarity matrix
    score = score + loc_sim

    car_lang_embeds = query_lang_embeds[query]
    if len(car_lang_embeds) >= 2:
        relation_sim = []
        other_lang_embed = car_lang_embeds[1]
        for _, car_vis_embeds in track_car_embeds.items():
            max_sim_val = 0
            for car_vis_embed in car_vis_embeds:
                max_sim_val = max(max_sim_val, np.matmul(other_lang_embed, car_vis_embed.T))
            relation_sim.append(max_sim_val)
        relation_sim = (relation_sim - np.min(relation_sim)) / (np.max(relation_sim) - np.min(relation_sim))
        # add relationship similarity matrix
        # the hyper-parameter here is set to 0.2, change it manually to adopt for your model
        score = score + relation_sim * 0.2

    index = np.argsort(score)[::-1]
    results[query] = []
    for i in index:
        results[query].append(tacks_ids[i])

with open("results_mergefinal.json", "w") as f:
    json.dump(results, f,indent=4)
